{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LDM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureMapper(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, dropout = 0.1):\n",
    "        super(FeatureMapper, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(16, self.embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feature_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_dim = 14\n",
    "n_epochs = 100\n",
    "Aij = torch.tensor([[0, 2, 0, 3, 1, 2, 0, 0, 2, 0, 1, 0], \n",
    "                    [0, 0, 2, 0, 1, 0, 3, 0, 0, 1, 0, 0],\n",
    "                    [3, 3, 0, 0, 0, 1, 0, 3, 0, 0, 0, 1],\n",
    "                    [3, 3, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0],\n",
    "                    [0, 0, 2, 0, 0, 0, 3, 0, 1, 0, 0, 0],\n",
    "                    [1, 2, 0, 3, 1, 2, 0, 0, 2, 0, 1, 0], \n",
    "                    [0, 0, 2, 0, 1, 0, 0, 1, 0, 1, 0, 0],\n",
    "                    [0, 3, 1, 0, 0, 1, 0, 3, 0, 0, 0, 1],\n",
    "                    [3, 3, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2],\n",
    "                    [0, 0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0]],dtype=torch.float32, device=device)\n",
    "lr = 0.01\n",
    "seed = 20\n",
    "ldm_trained = LDM(Aij, embedding_dim, device, n_epochs, lr, seed)\n",
    "ldm_trained.train()\n",
    "Aij_probs_true = ldm_trained.probit()  # Compute the probit probability matrix\n",
    "loss_out = ldm_trained.train()\n",
    "w, v = ldm_trained.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aij_idx: [0]. There are 10 drugs in Aij and 1 drugs in the index.\n",
      "f_vec_idx: [0 0 1 1 2 3 4 5 6 7 8 9]. There are 12 drugs in f_vec and 12 drugs in the index.\n"
     ]
    }
   ],
   "source": [
    "f_vec = torch.tensor([[0, 1, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0],\n",
    "                      [0, 0, 1, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0],\n",
    "                      [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 300, 0, 0, 0], \n",
    "                      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 300, 0, 0, 0], \n",
    "                      [0, 0, 0, 0, 1, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0], \n",
    "                      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 40, 0], \n",
    "                      [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 25, 0, 0],\n",
    "                      [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 30, 0, 0, 0, 0], \n",
    "                      [0, 0, 0, 1, 0, 0, 0, 0, 47, 0, 0, 0, 0, 0, 0], \n",
    "                      [0, 1, 0, 0, 0, 0, 34, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "                      [1, 0, 0, 0, 0, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                      [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17]\n",
    "                      ], dtype=torch.float32, device=device)\n",
    "adj_matrix_idx = pd.read_csv('/Users/christine/LatentDistanceModel/data/adj_matrix_idx.csv', sep =',')\n",
    "a_names = adj_matrix_idx[0:Aij.shape[0]]\n",
    "a_idx = {drug_id: idx for idx, drug_id in enumerate(a_names)}\n",
    "a_idx = np.array([a_idx[drug_id] for drug_id in a_names])\n",
    "f_vec_idx = np.array([0,0 , 1,1,  2, 3, 4, 5, 6, 7, 8, 9])\n",
    "print(f'Aij_idx: {a_idx}. There are {Aij.shape[0]} drugs in Aij and {len(a_idx)} drugs in the index.\\nf_vec_idx: {f_vec_idx}. There are {f_vec.shape[0]} drugs in f_vec and {len(f_vec_idx)} drugs in the index.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([0, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_idx, f_vec_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.4708\n",
      "Epoch 1, Loss: 1.4316\n",
      "Epoch 2, Loss: 1.1256\n",
      "Epoch 3, Loss: 1.1074\n",
      "Epoch 4, Loss: 1.0076\n",
      "Epoch 5, Loss: 0.9700\n",
      "Epoch 6, Loss: 1.0012\n",
      "Epoch 7, Loss: 0.9267\n",
      "Epoch 8, Loss: 0.9377\n",
      "Epoch 9, Loss: 0.9270\n",
      "Epoch 10, Loss: 0.9738\n",
      "Epoch 11, Loss: 0.9413\n",
      "Epoch 12, Loss: 0.8992\n",
      "Epoch 13, Loss: 0.9235\n",
      "Epoch 14, Loss: 0.9703\n",
      "Epoch 15, Loss: 0.9127\n",
      "Epoch 16, Loss: 0.8497\n",
      "Epoch 17, Loss: 0.8636\n",
      "Epoch 18, Loss: 0.8517\n",
      "Epoch 19, Loss: 0.8328\n",
      "Epoch 20, Loss: 0.7894\n",
      "Epoch 21, Loss: 0.8503\n",
      "Epoch 22, Loss: 0.8208\n",
      "Epoch 23, Loss: 0.8155\n",
      "Epoch 24, Loss: 0.7675\n",
      "Epoch 25, Loss: 0.8518\n",
      "Epoch 26, Loss: 0.9334\n",
      "Epoch 27, Loss: 0.8464\n",
      "Epoch 28, Loss: 0.8073\n",
      "Epoch 29, Loss: 0.8324\n",
      "Epoch 30, Loss: 0.7798\n",
      "Epoch 31, Loss: 0.7914\n",
      "Epoch 32, Loss: 0.7120\n",
      "Epoch 33, Loss: 0.6943\n",
      "Epoch 34, Loss: 0.7553\n",
      "Epoch 35, Loss: 0.7029\n",
      "Epoch 36, Loss: 0.7217\n",
      "Epoch 37, Loss: 0.7539\n",
      "Epoch 38, Loss: 0.6656\n",
      "Epoch 39, Loss: 0.6935\n",
      "Epoch 40, Loss: 0.8399\n",
      "Epoch 41, Loss: 0.7595\n",
      "Epoch 42, Loss: 0.6322\n",
      "Epoch 43, Loss: 0.6453\n",
      "Epoch 44, Loss: 0.6060\n",
      "Epoch 45, Loss: 0.6455\n",
      "Epoch 46, Loss: 0.7113\n",
      "Epoch 47, Loss: 0.5939\n",
      "Epoch 48, Loss: 0.6008\n",
      "Epoch 49, Loss: 0.7319\n",
      "Epoch 50, Loss: 0.5758\n",
      "Epoch 51, Loss: 0.6170\n",
      "Epoch 52, Loss: 0.6530\n",
      "Epoch 53, Loss: 0.6705\n",
      "Epoch 54, Loss: 0.5818\n",
      "Epoch 55, Loss: 0.5278\n",
      "Epoch 56, Loss: 0.5423\n",
      "Epoch 57, Loss: 0.6594\n",
      "Epoch 58, Loss: 0.5900\n",
      "Epoch 59, Loss: 0.5972\n",
      "Epoch 60, Loss: 0.5926\n",
      "Epoch 61, Loss: 0.5588\n",
      "Epoch 62, Loss: 0.5739\n",
      "Epoch 63, Loss: 0.5287\n",
      "Epoch 64, Loss: 0.5432\n",
      "Epoch 65, Loss: 0.5538\n",
      "Epoch 66, Loss: 0.6558\n",
      "Epoch 67, Loss: 0.5255\n",
      "Epoch 68, Loss: 0.5266\n",
      "Epoch 69, Loss: 0.5090\n",
      "Epoch 70, Loss: 0.6255\n",
      "Epoch 71, Loss: 0.6419\n",
      "Epoch 72, Loss: 0.5031\n",
      "Epoch 73, Loss: 0.4516\n",
      "Epoch 74, Loss: 0.5387\n",
      "Epoch 75, Loss: 0.5756\n",
      "Epoch 76, Loss: 0.5522\n",
      "Epoch 77, Loss: 0.5530\n",
      "Epoch 78, Loss: 0.5746\n",
      "Epoch 79, Loss: 0.4660\n",
      "Epoch 80, Loss: 0.4462\n",
      "Epoch 81, Loss: 0.4169\n",
      "Epoch 82, Loss: 0.4408\n",
      "Epoch 83, Loss: 0.5213\n",
      "Epoch 84, Loss: 0.5630\n",
      "Epoch 85, Loss: 0.5158\n",
      "Epoch 86, Loss: 0.4243\n",
      "Epoch 87, Loss: 0.4301\n",
      "Epoch 88, Loss: 0.4098\n",
      "Epoch 89, Loss: 0.4969\n",
      "Epoch 90, Loss: 0.4583\n",
      "Epoch 91, Loss: 0.5283\n",
      "Epoch 92, Loss: 0.4434\n",
      "Epoch 93, Loss: 0.4544\n",
      "Epoch 94, Loss: 0.5049\n",
      "Epoch 95, Loss: 0.5323\n",
      "Epoch 96, Loss: 0.5876\n",
      "Epoch 97, Loss: 0.3784\n",
      "Epoch 98, Loss: 0.3549\n",
      "Epoch 99, Loss: 0.6467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/td49stzx1sb1x3vskq7xg2540000gn/T/ipykernel_69771/3297555768.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  f_vec_tensor = torch.tensor(f_vec, dtype=torch.float32)\n",
      "/var/folders/nd/td49stzx1sb1x3vskq7xg2540000gn/T/ipykernel_69771/3297555768.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_tensor = torch.tensor(w, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "mapper = FeatureMapper(input_dim=f_vec.shape[1], embedding_dim=w.shape[1])\n",
    "optimizer = torch.optim.Adam(mapper.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "num_epochs = 100\n",
    "\n",
    "w_frozen = ldm_trained.w.detach().clone()\n",
    "f_vec_tensor = torch.tensor(f_vec, dtype=torch.float32)\n",
    "drug_idx_tensor = torch.tensor(f_vec_idx, dtype=torch.long)\n",
    "w_tensor = torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    mapper.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z_pred = mapper(f_vec_tensor)\n",
    "    z_true = w_tensor[drug_idx_tensor]\n",
    "    loss = loss_fn(z_pred, z_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 14])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_tensor[drug_idx_tensor].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([745, 3677])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Chewing gum</th>\n",
       "      <th>Inhal</th>\n",
       "      <th>Inhal.aerosol</th>\n",
       "      <th>Inhal.powder</th>\n",
       "      <th>Inhal.solution</th>\n",
       "      <th>N</th>\n",
       "      <th>O</th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>...</th>\n",
       "      <th>V08AB05</th>\n",
       "      <th>V08AB06</th>\n",
       "      <th>V08AB07</th>\n",
       "      <th>V08AB09</th>\n",
       "      <th>V08CA03</th>\n",
       "      <th>V08CA04</th>\n",
       "      <th>V08CA06</th>\n",
       "      <th>V08CA08</th>\n",
       "      <th>V08CA09</th>\n",
       "      <th>V09AB03</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1095 rows Ã— 1011 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0  Chewing gum  Inhal  Inhal.aerosol  Inhal.powder  Inhal.solution  \\\n",
       "0     0.0          0.0    0.0            0.0           0.0             0.0   \n",
       "1     0.0          0.0    0.0            0.0           0.0             0.0   \n",
       "2     0.0          0.0    0.0            0.0           0.0             0.0   \n",
       "3     0.0          0.0    0.0            0.0           0.0             0.0   \n",
       "4     0.0          0.0    0.0            0.0           0.0             0.0   \n",
       "...   ...          ...    ...            ...           ...             ...   \n",
       "1090  0.0          0.0    0.0            0.0           0.0             0.0   \n",
       "1091  0.0          0.0    0.0            0.0           0.0             0.0   \n",
       "1092  0.0          0.0    0.0            0.0           0.0             0.0   \n",
       "1093  0.0          0.0    0.0            0.0           0.0             0.0   \n",
       "1094  0.0          0.0    0.0            0.0           0.0             0.0   \n",
       "\n",
       "        N    O    P    R  ...  V08AB05  V08AB06  V08AB07  V08AB09  V08CA03  \\\n",
       "0     0.0  1.0  0.0  0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "1     0.0  0.0  1.0  0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "2     0.0  0.0  0.0  0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "3     0.0  1.0  0.0  0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "4     0.0  0.0  1.0  0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "...   ...  ...  ...  ...  ...      ...      ...      ...      ...      ...   \n",
       "1090  0.0  1.0  0.0  0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "1091  0.0  0.0  1.0  0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "1092  0.0  0.0  0.0  0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "1093  0.0  1.0  0.0  0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "1094  0.0  0.0  1.0  0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "      V08CA04  V08CA06  V08CA08  V08CA09  V09AB03  \n",
       "0         0.0      0.0      0.0      0.0      0.0  \n",
       "1         0.0      0.0      0.0      0.0      0.0  \n",
       "2         0.0      0.0      0.0      0.0      0.0  \n",
       "3         0.0      0.0      0.0      0.0      0.0  \n",
       "4         0.0      0.0      0.0      0.0      0.0  \n",
       "...       ...      ...      ...      ...      ...  \n",
       "1090      0.0      0.0      0.0      0.0      0.0  \n",
       "1091      0.0      0.0      0.0      0.0      0.0  \n",
       "1092      0.0      0.0      0.0      0.0      0.0  \n",
       "1093      0.0      0.0      0.0      0.0      0.0  \n",
       "1094      0.0      0.0      0.0      0.0      0.0  \n",
       "\n",
       "[1095 rows x 1011 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(path_to_csv, device):\n",
    "    df = pd.read_csv(path_to_csv, index_col=0)\n",
    "    Aij = torch.tensor(df.values, dtype=torch.float32).to(device)\n",
    "    return Aij\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "csv_path = \"/Users/christine/LatentDistanceModel/data/filtered_adj_matrix.csv\" \n",
    "Aij_real = load_data(csv_path, device)\n",
    "print(Aij_real.shape)\n",
    "\n",
    "#importing the data\n",
    "feature_vector = pd.read_csv('/Users/christine/LatentDistanceModel/data/feature_vector.tsv', sep='\\t')\n",
    "feature_vector.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['CID100000085', 'CID100000137', 'CID100000143', 'CID100000158',\n",
       "        'CID100000159', 'CID100000160', 'CID100000191', 'CID100000214',\n",
       "        'CID100000232', 'CID100000247', 'CID100000271', 'CID100000311',\n",
       "        'CID100000444', 'CID100000450', 'CID100000453', 'CID100000581',\n",
       "        'CID100000596', 'CID100000598', 'CID100000727', 'CID100000738',\n",
       "        'CID100000750', 'CID100000772', 'CID100000861', 'CID100000937',\n",
       "        'CID100000942', 'CID100001065', 'CID100001125', 'CID100001134',\n",
       "        'CID100001301', 'CID100001546', 'CID100001690', 'CID100001775',\n",
       "        'CID100001805', 'CID100001971', 'CID100001972', 'CID100001978',\n",
       "        'CID100001983', 'CID100001990', 'CID100002019', 'CID100002022',\n",
       "        'CID100002082', 'CID100002083', 'CID100002088', 'CID100002092',\n",
       "        'CID100002094', 'CID100002099', 'CID100002118', 'CID100002130',\n",
       "        'CID100002153', 'CID100002156', 'CID100002159', 'CID100002160',\n",
       "        'CID100002161', 'CID100002162', 'CID100002170', 'CID100002171',\n",
       "        'CID100002177', 'CID100002179', 'CID100002182', 'CID100002187',\n",
       "        'CID100002215', 'CID100002216', 'CID100002232', 'CID100002249',\n",
       "        'CID100002250', 'CID100002265', 'CID100002266', 'CID100002267',\n",
       "        'CID100002269', 'CID100002274', 'CID100002284', 'CID100002311',\n",
       "        'CID100002351', 'CID100002366', 'CID100002369', 'CID100002370',\n",
       "        'CID100002375', 'CID100002405', 'CID100002435', 'CID100002441',\n",
       "        'CID100002443', 'CID100002462', 'CID100002471', 'CID100002474',\n",
       "        'CID100002476', 'CID100002477', 'CID100002478', 'CID100002487',\n",
       "        'CID100002512', 'CID100002519', 'CID100002520', 'CID100002522',\n",
       "        'CID100002524', 'CID100002541', 'CID100002548', 'CID100002550',\n",
       "        'CID100002554', 'CID100002576', 'CID100002578', 'CID100002585',\n",
       "        'CID100002609', 'CID100002617', 'CID100002622', 'CID100002631',\n",
       "        'CID100002646', 'CID100002650', 'CID100002654', 'CID100002656',\n",
       "        'CID100002658', 'CID100002662', 'CID100002675', 'CID100002676',\n",
       "        'CID100002678', 'CID100002684', 'CID100002708', 'CID100002713',\n",
       "        'CID100002732', 'CID100002749', 'CID100002751', 'CID100002756',\n",
       "        'CID100002764', 'CID100002767', 'CID100002771', 'CID100002786',\n",
       "        'CID100002789', 'CID100002792', 'CID100002800', 'CID100002801',\n",
       "        'CID100002802', 'CID100002803', 'CID100002806', 'CID100002818',\n",
       "        'CID100002833', 'CID100002881', 'CID100002895', 'CID100002907',\n",
       "        'CID100002909', 'CID100002949', 'CID100002951', 'CID100002955',\n",
       "        'CID100002958', 'CID100002972', 'CID100002973', 'CID100002978',\n",
       "        'CID100003000', 'CID100003003', 'CID100003007', 'CID100003008',\n",
       "        'CID100003009', 'CID100003015', 'CID100003016', 'CID100003032',\n",
       "        'CID100003040', 'CID100003042', 'CID100003043', 'CID100003059',\n",
       "        'CID100003062', 'CID100003063', 'CID100003066', 'CID100003075',\n",
       "        'CID100003108', 'CID100003114', 'CID100003121', 'CID100003143',\n",
       "        'CID100003148', 'CID100003151', 'CID100003152', 'CID100003154',\n",
       "        'CID100003157', 'CID100003158', 'CID100003199', 'CID100003203',\n",
       "        'CID100003219', 'CID100003222', 'CID100003241', 'CID100003261',\n",
       "        'CID100003305', 'CID100003308', 'CID100003310', 'CID100003324',\n",
       "        'CID100003325', 'CID100003331', 'CID100003333', 'CID100003339',\n",
       "        'CID100003342', 'CID100003345', 'CID100003348', 'CID100003350',\n",
       "        'CID100003355', 'CID100003364', 'CID100003365', 'CID100003367',\n",
       "        'CID100003373', 'CID100003379', 'CID100003380', 'CID100003381',\n",
       "        'CID100003382', 'CID100003384', 'CID100003385', 'CID100003386',\n",
       "        'CID100003392', 'CID100003397', 'CID100003403', 'CID100003404',\n",
       "        'CID100003406', 'CID100003410', 'CID100003414', 'CID100003417',\n",
       "        'CID100003419', 'CID100003440', 'CID100003443', 'CID100003449',\n",
       "        'CID100003454', 'CID100003461', 'CID100003462', 'CID100003463',\n",
       "        'CID100003475', 'CID100003476', 'CID100003478', 'CID100003494',\n",
       "        'CID100003510', 'CID100003519', 'CID100003559', 'CID100003637',\n",
       "        'CID100003639', 'CID100003640', 'CID100003648', 'CID100003657',\n",
       "        'CID100003661', 'CID100003672', 'CID100003676', 'CID100003685',\n",
       "        'CID100003690', 'CID100003696', 'CID100003702', 'CID100003706',\n",
       "        'CID100003715', 'CID100003724', 'CID100003730', 'CID100003734',\n",
       "        'CID100003736', 'CID100003738', 'CID100003741', 'CID100003746',\n",
       "        'CID100003749', 'CID100003759', 'CID100003763', 'CID100003784',\n",
       "        'CID100003793', 'CID100003823', 'CID100003825', 'CID100003826',\n",
       "        'CID100003869', 'CID100003877', 'CID100003878', 'CID100003883',\n",
       "        'CID100003890', 'CID100003902', 'CID100003911', 'CID100003915',\n",
       "        'CID100003929', 'CID100003937', 'CID100003948', 'CID100003954',\n",
       "        'CID100003957', 'CID100003958', 'CID100003961', 'CID100003962',\n",
       "        'CID100003964', 'CID100003998', 'CID100004011', 'CID100004033',\n",
       "        'CID100004036', 'CID100004046', 'CID100004051', 'CID100004053',\n",
       "        'CID100004054', 'CID100004075', 'CID100004086', 'CID100004091',\n",
       "        'CID100004112', 'CID100004114', 'CID100004140', 'CID100004158',\n",
       "        'CID100004168', 'CID100004170', 'CID100004171', 'CID100004173',\n",
       "        'CID100004174', 'CID100004178', 'CID100004184', 'CID100004189',\n",
       "        'CID100004192', 'CID100004195', 'CID100004196', 'CID100004197',\n",
       "        'CID100004205', 'CID100004211', 'CID100004212', 'CID100004236',\n",
       "        'CID100004248', 'CID100004253', 'CID100004259', 'CID100004264',\n",
       "        'CID100004272', 'CID100004409', 'CID100004411', 'CID100004419',\n",
       "        'CID100004428', 'CID100004436', 'CID100004440', 'CID100004443',\n",
       "        'CID100004449', 'CID100004451', 'CID100004456', 'CID100004463',\n",
       "        'CID100004473', 'CID100004485', 'CID100004493', 'CID100004497',\n",
       "        'CID100004499', 'CID100004509', 'CID100004510', 'CID100004513',\n",
       "        'CID100004539', 'CID100004542', 'CID100004543', 'CID100004547',\n",
       "        'CID100004583', 'CID100004585', 'CID100004594', 'CID100004595',\n",
       "        'CID100004599', 'CID100004603', 'CID100004609', 'CID100004623',\n",
       "        'CID100004634', 'CID100004635', 'CID100004666', 'CID100004673',\n",
       "        'CID100004675', 'CID100004679', 'CID100004691', 'CID100004724',\n",
       "        'CID100004725', 'CID100004727', 'CID100004739', 'CID100004740',\n",
       "        'CID100004745', 'CID100004775', 'CID100004810', 'CID100004812',\n",
       "        'CID100004819', 'CID100004828', 'CID100004829', 'CID100004834',\n",
       "        'CID100004845', 'CID100004865', 'CID100004885', 'CID100004889',\n",
       "        'CID100004891', 'CID100004893', 'CID100004915', 'CID100004920',\n",
       "        'CID100004923', 'CID100004932', 'CID100004946', 'CID100004991',\n",
       "        'CID100004999', 'CID100005002', 'CID100005005', 'CID100005029',\n",
       "        'CID100005035', 'CID100005038', 'CID100005039', 'CID100005051',\n",
       "        'CID100005070', 'CID100005071', 'CID100005073', 'CID100005076',\n",
       "        'CID100005077', 'CID100005078', 'CID100005090', 'CID100005095',\n",
       "        'CID100005152', 'CID100005155', 'CID100005184', 'CID100005195',\n",
       "        'CID100005203', 'CID100005206', 'CID100005210', 'CID100005212',\n",
       "        'CID100005245', 'CID100005253', 'CID100005257', 'CID100005297',\n",
       "        'CID100005300', 'CID100005352', 'CID100005358', 'CID100005372',\n",
       "        'CID100005376', 'CID100005379', 'CID100005381', 'CID100005391',\n",
       "        'CID100005394', 'CID100005396', 'CID100005401', 'CID100005402',\n",
       "        'CID100005403', 'CID100005404', 'CID100005408', 'CID100005426',\n",
       "        'CID100005452', 'CID100005453', 'CID100005466', 'CID100005468',\n",
       "        'CID100005478', 'CID100005479', 'CID100005486', 'CID100005487',\n",
       "        'CID100005496', 'CID100005508', 'CID100005512', 'CID100005514',\n",
       "        'CID100005516', 'CID100005523', 'CID100005525', 'CID100005526',\n",
       "        'CID100005530', 'CID100005533', 'CID100005538', 'CID100005544',\n",
       "        'CID100005556', 'CID100005578', 'CID100005582', 'CID100005591',\n",
       "        'CID100005595', 'CID100005596', 'CID100005625', 'CID100005636',\n",
       "        'CID100005645', 'CID100005647', 'CID100005650', 'CID100005651',\n",
       "        'CID100005656', 'CID100005665', 'CID100005672', 'CID100005717',\n",
       "        'CID100005719', 'CID100005721', 'CID100005726', 'CID100005731',\n",
       "        'CID100005732', 'CID100005734', 'CID100005735', 'CID100005746',\n",
       "        'CID100005771', 'CID100005775', 'CID100005978', 'CID100006018',\n",
       "        'CID100006058', 'CID100006238', 'CID100006256', 'CID100007187',\n",
       "        'CID100009433', 'CID100010413', 'CID100010631', 'CID100012536',\n",
       "        'CID100012555', 'CID100012620', 'CID100014888', 'CID100016230',\n",
       "        'CID100016362', 'CID100016850', 'CID100016886', 'CID100018140',\n",
       "        'CID100019090', 'CID100022258', 'CID100022318', 'CID100025419',\n",
       "        'CID100027661', 'CID100027991', 'CID100027993', 'CID100030623',\n",
       "        'CID100032797', 'CID100034312', 'CID100036523', 'CID100037392',\n",
       "        'CID100037720', 'CID100038904', 'CID100039042', 'CID100039860',\n",
       "        'CID100040703', 'CID100040976', 'CID100041317', 'CID100041684',\n",
       "        'CID100041693', 'CID100041774', 'CID100041781', 'CID100042113',\n",
       "        'CID100042615', 'CID100042955', 'CID100044564', 'CID100047319',\n",
       "        'CID100047320', 'CID100047528', 'CID100047640', 'CID100047725',\n",
       "        'CID100050294', 'CID100051263', 'CID100051577', 'CID100051634',\n",
       "        'CID100054313', 'CID100054373', 'CID100054454', 'CID100054547',\n",
       "        'CID100054688', 'CID100054786', 'CID100054808', 'CID100054840',\n",
       "        'CID100055480', 'CID100056338', 'CID100056959', 'CID100057469',\n",
       "        'CID100057537', 'CID100059708', 'CID100059768', 'CID100060146',\n",
       "        'CID100060164', 'CID100060184', 'CID100060198', 'CID100060612',\n",
       "        'CID100060613', 'CID100060695', 'CID100060706', 'CID100060714',\n",
       "        'CID100060726', 'CID100060751', 'CID100060752', 'CID100060754',\n",
       "        'CID100060787', 'CID100060795', 'CID100060814', 'CID100060830',\n",
       "        'CID100060834', 'CID100060843', 'CID100060852', 'CID100060853',\n",
       "        'CID100060864', 'CID100060867', 'CID100060871', 'CID100060877',\n",
       "        'CID100060878', 'CID100060953', 'CID100062819', 'CID100062924',\n",
       "        'CID100062959', 'CID100064147', 'CID100065014', 'CID100065628',\n",
       "        'CID100065856', 'CID100065863', 'CID100065866', 'CID100065999',\n",
       "        'CID100068613', 'CID100068740', 'CID100068844', 'CID100071158',\n",
       "        'CID100071273', 'CID100071301', 'CID100071316', 'CID100071329',\n",
       "        'CID100071348', 'CID100071406', 'CID100071616', 'CID100072054',\n",
       "        'CID100072081', 'CID100072111', 'CID100072466', 'CID100072938',\n",
       "        'CID100073303', 'CID100073658', 'CID100074989', 'CID100077992',\n",
       "        'CID100077993', 'CID100077996', 'CID100077998', 'CID100082148',\n",
       "        'CID100096312', 'CID100102258', 'CID100104741', 'CID100104758',\n",
       "        'CID100104799', 'CID100104849', 'CID100104865', 'CID100110634',\n",
       "        'CID100110635', 'CID100114709', 'CID100115237', 'CID100115355',\n",
       "        'CID100119182', 'CID100119607', 'CID100119830', 'CID100121396',\n",
       "        'CID100121749', 'CID100122197', 'CID100122316', 'CID100123015',\n",
       "        'CID100123597', 'CID100123606', 'CID100123610', 'CID100123611',\n",
       "        'CID100123619', 'CID100123620', 'CID100123623', 'CID100124087',\n",
       "        'CID100125017', 'CID100127909', 'CID100129228', 'CID100129806',\n",
       "        'CID100131535', 'CID100132804', 'CID100134018', 'CID100145068',\n",
       "        'CID100147912', 'CID100148121', 'CID100148192', 'CID100148211',\n",
       "        'CID100150310', 'CID100150311', 'CID100150610', 'CID100151075',\n",
       "        'CID100151165', 'CID100151171', 'CID100152945', 'CID100153941',\n",
       "        'CID100154058', 'CID100154256', 'CID100156326', 'CID100156418',\n",
       "        'CID100157688', 'CID100157920', 'CID100157921', 'CID100160051',\n",
       "        'CID100163296', 'CID100166548', 'CID100168924', 'CID100170361',\n",
       "        'CID100193962', 'CID100197281', 'CID100197712', 'CID100208902',\n",
       "        'CID100213023', 'CID100213039', 'CID100216209', 'CID100216235',\n",
       "        'CID100216237', 'CID100216326', 'CID100219024', 'CID100219078',\n",
       "        'CID100358641', 'CID100441332', 'CID100444006', 'CID100444013',\n",
       "        'CID100444033', 'CID100449193', 'CID100477468', 'CID100483407',\n",
       "        'CID100656628', 'CID100667490', 'CID103055172', 'CID103081362',\n",
       "        'CID103081884', 'CID103085017', 'CID103086257', 'CID104369359',\n",
       "        'CID104474778', 'CID104479094', 'CID104479097', 'CID104659568',\n",
       "        'CID104659569', 'CID105251896', 'CID105281007', 'CID105353980',\n",
       "        'CID105361912', 'CID105361917', 'CID105362070', 'CID105362420',\n",
       "        'CID105462337', 'CID105479141', 'CID105493381', 'CID106102852',\n",
       "        'CID106333887', 'CID106398970', 'CID106433091', 'CID106433101',\n",
       "        'CID106433117', 'CID106435110', 'CID106445540', 'CID106850789',\n",
       "        'CID106918182', 'CID106918456', 'CID106918462', 'CID106918558',\n",
       "        'CID109812414', 'CID109846180', 'CID109854489', 'CID111163584',\n",
       "        'CID111235728', 'CID116129616', 'CID116129629', 'CID116129632',\n",
       "        'CID116129665', 'CID116129672', 'CID116129682', 'CID116129690',\n",
       "        'CID116129703', 'CID116129704', 'CID116130199', 'CID116130957',\n",
       "        'CID116131215', 'CID116132265', 'CID116132418', 'CID116132438',\n",
       "        'CID116132441', 'CID116132446', 'CID116136245', 'CID116137271',\n",
       "        'CID116213095', 'CID122834577', 'CID123690938', 'CID125074470',\n",
       "        'CID125074886', 'CID125077405', 'CID125880656', 'CID126275995',\n",
       "        'CID144146714', 'CID144567678', 'CID151508717', 'CID153477714',\n",
       "        'CID154681041', 'CID154687131', 'CID156603655', 'CID170695640',\n",
       "        'CID171306834'], dtype=object),\n",
       " array(['CID100000085', 'CID100000085', 'CID100000137', ...,\n",
       "        'CID156603655', 'CID170695640', 'CID171306834'],\n",
       "       shape=(1095,), dtype=object))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find indexing for the two vectors\n",
    "adj_matrix_names = pd.read_csv('/Users/christine/LatentDistanceModel/data/filtered_adj_matrix.csv', sep=',')\n",
    "adj_matrix_idx = adj_matrix_names['Stitch flat']\n",
    "adj_matrix_idx.to_csv('/Users/christine/LatentDistanceModel/data/adj_matrix_idx.csv', index=False)\n",
    "adj_matrix_idx = adj_matrix_idx.to_numpy()\n",
    "\n",
    "#index of feature vector\n",
    "feature_vector_names = pd.read_csv('/Users/christine/LatentDistanceModel/data/feature_vector_names.tsv', sep = '\\t', )\n",
    "feature_vector_names['ID Adm.Rs'] = feature_vector_names['ID Adm.Rs'].str.split('_').str[0]\n",
    "feature_vector_idx = feature_vector_names['ID Adm.Rs'].to_numpy()\n",
    "adj_matrix_idx, feature_vector_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "        247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "        260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "        273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "        286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "        299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "        325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "        338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "        351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "        377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "        390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "        403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "        416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "        429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "        442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "        455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "        481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "        494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "        507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
       "        520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "        533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
       "        546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
       "        559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
       "        572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
       "        585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
       "        598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
       "        611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
       "        624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n",
       "        637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n",
       "        650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662,\n",
       "        663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675,\n",
       "        676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688,\n",
       "        689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701,\n",
       "        702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714,\n",
       "        715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727,\n",
       "        728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740,\n",
       "        741, 742, 743, 744]),\n",
       " array([  0,   0,   1, ..., 742, 743, 744], shape=(1095,)))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for real data\n",
    "Aij_dic = {drug_id: idx for idx, drug_id in enumerate(adj_matrix_idx)}\n",
    "unique_drugs_Aij = pd.unique(adj_matrix_idx)\n",
    "Aij_idx = np.array([Aij_dic[drug_id] for drug_id in adj_matrix_names['Stitch flat']])\n",
    "\n",
    "#for feature vector\n",
    "unique_drugs_f = pd.unique(feature_vector_idx)\n",
    "f_dic = {drug_id: idx for idx, drug_id in enumerate(unique_drugs_f)}\n",
    "f_idx = np.array([f_dic[drug_id] for drug_id in feature_vector_names['ID Adm.Rs']])\n",
    "Aij_idx, f_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only in Aij: set()\n",
      "Only in feature vector: set()\n",
      "Not in either: set()\n",
      "0 are missing from feature vector\n"
     ]
    }
   ],
   "source": [
    "only_in_Aij = set(unique_drugs_Aij) - set(unique_drugs_f)\n",
    "only_in_f = set(unique_drugs_f) - set(unique_drugs_Aij)\n",
    "not_in_both = only_in_Aij.union(only_in_f)\n",
    "print(f\"Only in Aij: {only_in_Aij}\\nOnly in feature vector: {only_in_f}\\nNot in either: {not_in_both}\")\n",
    "print(f\"{len(only_in_Aij)} are missing from feature vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to tensor\n",
    "feature_tensor = torch.tensor(feature_vector.astype(np.float32).to_numpy(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_dim = 140\n",
    "lr = 0.01\n",
    "seed = 20\n",
    "ldm_trained_r = LDM(Aij_real, embedding_dim, device, n_epochs, lr, seed)\n",
    "ldm_trained_r.train()\n",
    "Aij_probs_true_r = ldm_trained_r.probit()  # Compute the probit probability matrix\n",
    "loss_out_r = ldm_trained_r.train()\n",
    "w_r, v_r = ldm_trained_r.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.6100\n",
      "Epoch 1, Loss: 3.3627\n",
      "Epoch 2, Loss: 2.8343\n",
      "Epoch 3, Loss: 1.9656\n",
      "Epoch 4, Loss: 1.7334\n",
      "Epoch 5, Loss: 1.5220\n",
      "Epoch 6, Loss: 1.3786\n",
      "Epoch 7, Loss: 1.2403\n",
      "Epoch 8, Loss: 1.1696\n",
      "Epoch 9, Loss: 1.1662\n",
      "Epoch 10, Loss: 1.1340\n",
      "Epoch 11, Loss: 1.0576\n",
      "Epoch 12, Loss: 1.0496\n",
      "Epoch 13, Loss: 1.1200\n",
      "Epoch 14, Loss: 1.0967\n",
      "Epoch 15, Loss: 1.0804\n",
      "Epoch 16, Loss: 1.0352\n",
      "Epoch 17, Loss: 1.0273\n",
      "Epoch 18, Loss: 1.0379\n",
      "Epoch 19, Loss: 1.0201\n",
      "Epoch 20, Loss: 1.0179\n",
      "Epoch 21, Loss: 1.0304\n",
      "Epoch 22, Loss: 1.0158\n",
      "Epoch 23, Loss: 1.0084\n",
      "Epoch 24, Loss: 1.0069\n",
      "Epoch 25, Loss: 1.0101\n",
      "Epoch 26, Loss: 1.0132\n",
      "Epoch 27, Loss: 1.0894\n",
      "Epoch 28, Loss: 1.0017\n",
      "Epoch 29, Loss: 1.0690\n",
      "Epoch 30, Loss: 1.0012\n",
      "Epoch 31, Loss: 1.0253\n",
      "Epoch 32, Loss: 1.0191\n",
      "Epoch 33, Loss: 1.0080\n",
      "Epoch 34, Loss: 1.0023\n",
      "Epoch 35, Loss: 1.0002\n",
      "Epoch 36, Loss: 0.9937\n",
      "Epoch 37, Loss: 0.9962\n",
      "Epoch 38, Loss: 0.9992\n",
      "Epoch 39, Loss: 0.9929\n",
      "Epoch 40, Loss: 1.0132\n",
      "Epoch 41, Loss: 0.9937\n",
      "Epoch 42, Loss: 0.9956\n",
      "Epoch 43, Loss: 0.9970\n",
      "Epoch 44, Loss: 0.9923\n",
      "Epoch 45, Loss: 0.9928\n",
      "Epoch 46, Loss: 1.0032\n",
      "Epoch 47, Loss: 0.9921\n",
      "Epoch 48, Loss: 0.9954\n",
      "Epoch 49, Loss: 0.9964\n",
      "Epoch 50, Loss: 0.9933\n",
      "Epoch 51, Loss: 0.9923\n",
      "Epoch 52, Loss: 0.9982\n",
      "Epoch 53, Loss: 0.9920\n",
      "Epoch 54, Loss: 0.9896\n",
      "Epoch 55, Loss: 0.9893\n",
      "Epoch 56, Loss: 0.9881\n",
      "Epoch 57, Loss: 0.9932\n",
      "Epoch 58, Loss: 0.9923\n",
      "Epoch 59, Loss: 0.9883\n",
      "Epoch 60, Loss: 0.9884\n",
      "Epoch 61, Loss: 0.9919\n",
      "Epoch 62, Loss: 0.9914\n",
      "Epoch 63, Loss: 0.9881\n",
      "Epoch 64, Loss: 0.9882\n",
      "Epoch 65, Loss: 0.9931\n",
      "Epoch 66, Loss: 0.9877\n",
      "Epoch 67, Loss: 0.9860\n",
      "Epoch 68, Loss: 0.9865\n",
      "Epoch 69, Loss: 0.9882\n",
      "Epoch 70, Loss: 0.9905\n",
      "Epoch 71, Loss: 0.9870\n",
      "Epoch 72, Loss: 0.9948\n",
      "Epoch 73, Loss: 0.9861\n",
      "Epoch 74, Loss: 0.9873\n",
      "Epoch 75, Loss: 0.9842\n",
      "Epoch 76, Loss: 0.9871\n",
      "Epoch 77, Loss: 0.9860\n",
      "Epoch 78, Loss: 0.9862\n",
      "Epoch 79, Loss: 0.9837\n",
      "Epoch 80, Loss: 0.9833\n",
      "Epoch 81, Loss: 0.9860\n",
      "Epoch 82, Loss: 0.9865\n",
      "Epoch 83, Loss: 0.9837\n",
      "Epoch 84, Loss: 0.9842\n",
      "Epoch 85, Loss: 0.9829\n",
      "Epoch 86, Loss: 0.9909\n",
      "Epoch 87, Loss: 0.9848\n",
      "Epoch 88, Loss: 0.9825\n",
      "Epoch 89, Loss: 0.9830\n",
      "Epoch 90, Loss: 0.9818\n",
      "Epoch 91, Loss: 0.9833\n",
      "Epoch 92, Loss: 0.9836\n",
      "Epoch 93, Loss: 0.9811\n",
      "Epoch 94, Loss: 0.9939\n",
      "Epoch 95, Loss: 0.9815\n",
      "Epoch 96, Loss: 0.9845\n",
      "Epoch 97, Loss: 0.9815\n",
      "Epoch 98, Loss: 0.9819\n",
      "Epoch 99, Loss: 0.9809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/td49stzx1sb1x3vskq7xg2540000gn/T/ipykernel_26104/2579014675.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_tensor = torch.tensor(w_r, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "mapper = FeatureMapper(input_dim=feature_vector.shape[1], embedding_dim=w_r.shape[1])\n",
    "optimizer = torch.optim.Adam(mapper.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "num_epochs = 100\n",
    "\n",
    "w_frozen = ldm_trained_r.w.detach().clone()\n",
    "feature_vec_tensor = torch.tensor(feature_vector.astype(np.float32).to_numpy(), dtype=torch.float32)\n",
    "feature_idx_tensor = torch.tensor(f_idx, dtype=torch.long)\n",
    "w_tensor = torch.tensor(w_r, dtype=torch.float32)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    mapper.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z_pred = mapper(feature_vec_tensor)\n",
    "    z_true = w_tensor[feature_idx_tensor]\n",
    "    loss = loss_fn(z_pred, z_true)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndToEnd(nn.Module):\n",
    "    def __init__(self, feature_mapper, v, gamma, beta, beta_thilde, a, b, Aij, device):\n",
    "        super().__init__()\n",
    "        self.feature_mapper = feature_mapper\n",
    "        self.v = nn.Parameter(v.clone())\n",
    "        self.gamma = nn.Parameter(gamma.clone()) \n",
    "        self.beta = nn.Parameter(beta.clone())  \n",
    "        self.beta_thilde = nn.Parameter(beta_thilde.clone())  \n",
    "        self.a = nn.Parameter(a.clone())\n",
    "        self.b = nn.Parameter(b.clone())\n",
    "\n",
    "        self.Aij = Aij\n",
    "        self.device = device\n",
    "        self.n_ordinal_classes = Aij.max().int().item() + 1\n",
    "        self.n_drugs, self.n_effects = Aij.shape\n",
    "\n",
    "    def get_thresholds(self):\n",
    "        # reused from LDM\n",
    "        deltas = torch.softmax(self.beta_thilde, dim = 0)  \n",
    "        thresholds = torch.cumsum(deltas, dim=0)* self.a - self.b\n",
    "        return torch.cat([torch.tensor([-float(\"inf\")], device=self.device), thresholds, torch.tensor([float(\"inf\")], device=self.device)])\n",
    "    \n",
    "    def forward(self, feature_vector):\n",
    "        w_pred = self.feature_mapper(feature_vector) #now what is being modelled is the features from the feature vec\n",
    "        normal_dist = Normal(0, 1) \n",
    "        probit_matrix = torch.zeros((self.n_ordinal_classes, self.n_drugs, self.n_effects), device=self.device)\n",
    "        thresholds = self.get_thresholds()\n",
    "    \n",
    "        #Linear term (\\beta^T x_{i,j})\n",
    "        linear_term = torch.matmul(self.Aij, self.beta.unsqueeze(1))\n",
    "\n",
    "        # Distance term -|w_i - v_j|\n",
    "        dist = -torch.norm(w_pred.unsqueeze(1) - self.v.unsqueeze(0), dim=2)\n",
    "\n",
    "        # Latent variable \\beta^T x_{i,j} + \\alpha(u_i - u_j)\n",
    "        latent_var = self.gamma + linear_term + dist\n",
    "        \n",
    "        for y in range(self.n_ordinal_classes):\n",
    "            z1 = latent_var - thresholds[y]\n",
    "            z2 = latent_var - thresholds[y+1]\n",
    "            probit_matrix[y, :, :] = normal_dist.cdf(z1) - normal_dist.cdf(z2)\n",
    "        return probit_matrix\n",
    "    \n",
    "    def ordinal_cross_entropy_loss(self, probit_matrix):\n",
    "    # Compute the predicted probabilities using the probit function\n",
    "\n",
    "        # Initialize loss variable\n",
    "        loss = 0.0\n",
    "\n",
    "        # Convert Aij to a one-hot encoded tensor\n",
    "        one_hot_target = torch.zeros(self.n_drugs, self.n_effects, self.n_ordinal_classes, device=self.device)\n",
    "        one_hot_target.scatter_(-1, self.Aij.unsqueeze(-1).long(), 1)  # One-hot encoding\n",
    "\n",
    "        # Compute the log-likelihood loss efficiently\n",
    "        prob = probit_matrix  # Shape: (n_ordinal_classes, n_drugs, n_effects)\n",
    "        loss = -torch.mean(torch.log(torch.sum(prob * one_hot_target.permute(2, 0, 1), dim=0) + 1e-8))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/td49stzx1sb1x3vskq7xg2540000gn/T/ipykernel_69771/294856734.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Aij_tensor = torch.tensor(Aij, dtype=torch.float32, device=device)\n",
      "/var/folders/nd/td49stzx1sb1x3vskq7xg2540000gn/T/ipykernel_69771/294856734.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  f_vec_tensor = torch.tensor(feat_vec, dtype=torch.float32, device=device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (12) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 25\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_vec_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mordinal_cross_entropy_loss(probs)\n\u001b[1;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[30], line 36\u001b[0m, in \u001b[0;36mEndToEnd.forward\u001b[0;34m(self, feature_vector)\u001b[0m\n\u001b[1;32m     33\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnorm(w_pred\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Latent variable \\beta^T x_{i,j} + \\alpha(u_i - u_j)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m latent_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlinear_term\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdist\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_ordinal_classes):\n\u001b[1;32m     39\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m latent_var \u001b[38;5;241m-\u001b[39m thresholds[y]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (12) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Aij_tensor = torch.tensor(Aij, dtype=torch.float32, device=device)\n",
    "feat_vec = f_vec[f_vec_idx]\n",
    "f_vec_tensor = torch.tensor(feat_vec, dtype=torch.float32, device=device)\n",
    "\n",
    "model = EndToEnd(\n",
    "    feature_mapper=FeatureMapper(f_vec.shape[1], embedding_dim=w.shape[1]).to(device),\n",
    "    v=ldm_trained.v,\n",
    "    gamma=ldm_trained.gamma,\n",
    "    beta=ldm_trained.beta,\n",
    "    beta_thilde=ldm_trained.beta_thilde,\n",
    "    a=ldm_trained.a,\n",
    "    b=ldm_trained.b,\n",
    "    Aij=Aij_tensor,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    probs = model(f_vec_tensor)\n",
    "    loss = model.ordinal_cross_entropy_loss(probs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
