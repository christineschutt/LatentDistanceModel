{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LDMprobit(torch.nn.Module):\n",
    "    def __init__(self, Aij, thresholds, embedding_dim, device, n_epochs, lr, seed=0):\n",
    "        super(LDMprobit, self).__init__()\n",
    "        self.Aij = Aij.to(device)\n",
    "        self.thresholds = thresholds.to(device)\n",
    "        self.device = device\n",
    "        self.n_drugs, self.n_effects = Aij.shape\n",
    "        self.n_ordinal_classes = (len(thresholds) - 1)\n",
    "\n",
    "        #set seed\n",
    "        self.seed = seed\n",
    "        self.__set_seed(seed)\n",
    "\n",
    "        #Variables for the learning process\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "\n",
    "        #parameters to be learned\n",
    "        self.beta = nn.Parameter(torch.randn(self.n_effects, device=device))\n",
    "        self.w = torch.nn.Parameter(torch.randn(self.n_drugs, embedding_dim))  # Latent embeddings for drugs\n",
    "        self.v = torch.nn.Parameter(torch.randn(self.n_effects, embedding_dim))  # Latent embeddings for side effects\n",
    "        self.beta_thilde = nn.Parameter(torch.randn(self.n_ordinal_classes, device=device))\n",
    "    \n",
    "    def __set_seed(self, seed):\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        return self.w, self.v\n",
    "    \n",
    "    def get_thresholds(self):\n",
    "        softmax_beta = torch.softmax(self.beta_thilde, dim=0)\n",
    "\n",
    "    def probit(self):\n",
    "        #n_ordinal_classes = len(self.thresholds) - 1\n",
    "        #n_drugs, n_effects = self.Aij.shape\n",
    "        normal_dist = Normal(0, 1) # Noise contaminated by normal distribution\n",
    "        probit_matrix = torch.zeros((self.n_drugs, self.n_effects, self.n_ordinal_classes), device=self.device)\n",
    "\n",
    "    \n",
    "        #Linear term (\\beta^T x_{i,j})\n",
    "        linear_term = torch.matmul(self.Aij, self.beta.unsqueeze(1))\n",
    "\n",
    "        # Distance term -|w_i - v_j|\n",
    "        dist = -torch.norm(self.w.unsqueeze(1) - self.v.unsqueeze(0), dim=2)\n",
    "\n",
    "        # Latent variable \\beta^T x_{i,j} + \\alpha(u_i - u_j)\n",
    "        latent_var = linear_term + dist\n",
    "        \n",
    "        for y in range(self.n_ordinal_classes):\n",
    "            z1 = latent_var - self.thresholds[y]\n",
    "            z2 = latent_var - self.thresholds[y+1]\n",
    "            probit_matrix[:, :, y] = normal_dist.cdf(z1) - normal_dist.cdf(z2)\n",
    "        return probit_matrix\n",
    "\n",
    "    def ordinal_cross_entropy_loss(self):\n",
    "    # Compute the predicted probabilities using the probit function\n",
    "        probit_matrix = self.probit() \n",
    "\n",
    "        # Initialize loss variable\n",
    "        loss = 0.0\n",
    "\n",
    "        # Iterate over all drugs and side effects\n",
    "        for i in range(self.n_drugs):  # For each drug\n",
    "            for j in range(self.n_effects):  # For each side effect\n",
    "                if self.Aij[i, j] != 0:  # Only compute loss for nonzero entries\n",
    "                    target_class = int(self.Aij[i, j]) - 1  # Convert severity to class (0-based)\n",
    "                    \n",
    "                    #One-hot encode target \n",
    "                    one_hot_target = torch.zeros(self.n_ordinal_classes, device=self.device)\n",
    "                    one_hot_target[target_class] = 1  # Set the correct class to 1\n",
    "\n",
    "                    # Compute the log-likelihood \n",
    "                    prob = probit_matrix[i, j]\n",
    "                    loss -= torch.log(torch.sum(prob * one_hot_target) + 1e-8)  # Negative log-likelihood, addition of small number to avoid log(0)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.probit()\n",
    "        \n",
    "    def learn(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            avg_loss = self.train_one_epoch(optimizer)\n",
    "            print(f\"Epoch {epoch+1}/{self.n_epochs}, Loss: {avg_loss:.4f}\")\n",
    "        return self.ordinal_cross_entropy_loss()\n",
    "\n",
    "    \n",
    "    def train_one_epoch(self, optimizer, batch_size=32):\n",
    "        total_loss = 0\n",
    "        n_batches = (self.n_drugs + batch_size - 1) // batch_size\n",
    "        for _ in range(n_batches):\n",
    "            batch_loss = self.train_one_batch(optimizer)\n",
    "            total_loss += batch_loss\n",
    "        return total_loss / n_batches\n",
    "    \n",
    "\n",
    "    def train_one_batch(self, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "        loss = self.ordinal_cross_entropy_loss()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.beta, self.w.detach().cpu(), self.v.detach().cpu()\n",
    "\n",
    "    def save_embeddings():\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(140.2836, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_dim = 4\n",
    "Aij = torch.tensor([[0, 2, 0, 3, 1, 2], \n",
    "                    [0, 0, 0, 0, 1, 2],\n",
    "                    [0, 0, 2, 0, 1, 2],\n",
    "                    [3, 3, 0, 0, 0, 1],\n",
    "                    [3, 3, 0, 0, 0, 1],\n",
    "                    [0, 0, 2, 0, 0, 0]],dtype=torch.float32) #column: side effect, row:drug, value: frequency (ordinal)\n",
    "thresholds = torch.tensor([0, 1, 2,3], dtype=torch.float32, device=device) \n",
    "n_epochs = 100\n",
    "lr = 0.01\n",
    "seed = 42\n",
    "model = LDMprobit(Aij, thresholds, embedding_dim, device, n_epochs, lr, seed)\n",
    "probit_output = model.probit()  # Compute the probit probability matrix\n",
    "loss_out = model.ordinal_cross_entropy_loss()  # Compute the ordinal cross-entropy loss\n",
    "print(loss_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probit Output Shape: torch.Size([6, 6, 3])\n",
      "tensor([[[4.9788e-04, 9.0003e-06, 5.9605e-08],\n",
      "         [1.5269e-03, 3.7998e-05, 3.5763e-07],\n",
      "         [1.9052e-02, 1.1198e-03, 2.5451e-05],\n",
      "         [1.2656e-02, 6.3485e-04, 1.2279e-05],\n",
      "         [6.7136e-04, 1.3202e-05, 8.9407e-08],\n",
      "         [2.6510e-02, 1.7850e-03, 4.6521e-05]],\n",
      "\n",
      "        [[8.6129e-06, 5.9605e-08, 0.0000e+00],\n",
      "         [3.2783e-07, 0.0000e+00, 0.0000e+00],\n",
      "         [6.9588e-05, 7.7486e-07, 0.0000e+00],\n",
      "         [1.5327e-04, 2.0564e-06, 0.0000e+00],\n",
      "         [6.1095e-06, 2.9802e-08, 0.0000e+00],\n",
      "         [4.3716e-03, 1.5101e-04, 2.0266e-06]],\n",
      "\n",
      "        [[7.9435e-04, 1.6361e-05, 1.1921e-07],\n",
      "         [6.7288e-04, 1.3232e-05, 8.9407e-08],\n",
      "         [1.5348e-04, 2.0564e-06, 0.0000e+00],\n",
      "         [3.2181e-03, 1.0064e-04, 1.2219e-06],\n",
      "         [6.9737e-06, 5.9605e-08, 0.0000e+00],\n",
      "         [1.2899e-03, 3.0547e-05, 2.6822e-07]],\n",
      "\n",
      "        [[7.4274e-02, 8.1477e-03, 3.4806e-04],\n",
      "         [2.1345e-01, 4.7558e-02, 4.1633e-03],\n",
      "         [2.2434e-02, 1.4089e-03, 3.4213e-05],\n",
      "         [3.4144e-02, 2.5658e-03, 7.4744e-05],\n",
      "         [3.2824e-03, 1.0332e-04, 1.2517e-06],\n",
      "         [2.2809e-02, 1.4423e-03, 3.5286e-05]],\n",
      "\n",
      "        [[3.2399e-01, 1.1730e-01, 1.6780e-02],\n",
      "         [7.5388e-04, 1.5289e-05, 1.1921e-07],\n",
      "         [5.2330e-04, 9.5963e-06, 5.9605e-08],\n",
      "         [5.3090e-02, 4.9019e-03, 1.7586e-04],\n",
      "         [1.1594e-01, 1.6465e-02, 9.1347e-04],\n",
      "         [1.1188e-02, 5.3599e-04, 9.9242e-06]],\n",
      "\n",
      "        [[2.4494e-03, 7.0333e-05, 7.7486e-07],\n",
      "         [1.9654e-03, 5.2750e-05, 5.3644e-07],\n",
      "         [1.2885e-02, 6.5070e-04, 1.2696e-05],\n",
      "         [6.2084e-04, 1.1951e-05, 8.9407e-08],\n",
      "         [6.4109e-02, 6.5083e-03, 2.5705e-04],\n",
      "         [2.6509e-03, 7.7993e-05, 8.9407e-07]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "model = LDMprobit(Aij, thresholds, embedding_dim, device, n_epochs, lr, seed).to(device)\n",
    "\n",
    "# Run a forward pass\n",
    "probit_output = model.forward()  # Should return a probability matrix\n",
    "print(\"\\nProbit Output Shape:\", probit_output.shape)  # Expect (n_drugs, n_effects, n_ordinal_classes)\n",
    "print(probit_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Loss: 176.02639770507812\n"
     ]
    }
   ],
   "source": [
    "loss = model.ordinal_cross_entropy_loss()\n",
    "print(\"\\nInitial Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 176.0264\n",
      "Epoch 2/100, Loss: 172.7043\n",
      "Epoch 3/100, Loss: 168.0842\n",
      "Epoch 4/100, Loss: 165.0133\n",
      "Epoch 5/100, Loss: 162.0621\n",
      "Epoch 6/100, Loss: 159.1644\n",
      "Epoch 7/100, Loss: 156.3022\n",
      "Epoch 8/100, Loss: 153.5302\n",
      "Epoch 9/100, Loss: 150.2718\n",
      "Epoch 10/100, Loss: 147.6735\n",
      "Epoch 11/100, Loss: 143.4125\n",
      "Epoch 12/100, Loss: 140.9678\n",
      "Epoch 13/100, Loss: 138.3568\n",
      "Epoch 14/100, Loss: 135.3163\n",
      "Epoch 15/100, Loss: 132.1927\n",
      "Epoch 16/100, Loss: 129.5783\n",
      "Epoch 17/100, Loss: 126.9500\n",
      "Epoch 18/100, Loss: 124.2507\n",
      "Epoch 19/100, Loss: 120.3727\n",
      "Epoch 20/100, Loss: 117.8882\n",
      "Epoch 21/100, Loss: 114.9585\n",
      "Epoch 22/100, Loss: 112.2982\n",
      "Epoch 23/100, Loss: 109.8339\n",
      "Epoch 24/100, Loss: 107.0345\n",
      "Epoch 25/100, Loss: 104.4933\n",
      "Epoch 26/100, Loss: 102.0181\n",
      "Epoch 27/100, Loss: 99.5922\n",
      "Epoch 28/100, Loss: 97.2574\n",
      "Epoch 29/100, Loss: 94.9764\n",
      "Epoch 30/100, Loss: 91.3880\n",
      "Epoch 31/100, Loss: 89.2669\n",
      "Epoch 32/100, Loss: 87.2227\n",
      "Epoch 33/100, Loss: 85.2425\n",
      "Epoch 34/100, Loss: 82.7717\n",
      "Epoch 35/100, Loss: 80.5749\n",
      "Epoch 36/100, Loss: 78.5410\n",
      "Epoch 37/100, Loss: 76.6238\n",
      "Epoch 38/100, Loss: 74.5309\n",
      "Epoch 39/100, Loss: 72.6384\n",
      "Epoch 40/100, Loss: 70.8099\n",
      "Epoch 41/100, Loss: 69.0577\n",
      "Epoch 42/100, Loss: 67.3470\n",
      "Epoch 43/100, Loss: 65.6720\n",
      "Epoch 44/100, Loss: 64.0605\n",
      "Epoch 45/100, Loss: 62.5059\n",
      "Epoch 46/100, Loss: 61.0068\n",
      "Epoch 47/100, Loss: 59.5624\n",
      "Epoch 48/100, Loss: 58.1535\n",
      "Epoch 49/100, Loss: 56.7980\n",
      "Epoch 50/100, Loss: 55.4897\n",
      "Epoch 51/100, Loss: 54.2242\n",
      "Epoch 52/100, Loss: 53.0053\n",
      "Epoch 53/100, Loss: 51.8289\n",
      "Epoch 54/100, Loss: 50.6937\n",
      "Epoch 55/100, Loss: 49.5990\n",
      "Epoch 56/100, Loss: 48.5421\n",
      "Epoch 57/100, Loss: 47.5232\n",
      "Epoch 58/100, Loss: 46.5413\n",
      "Epoch 59/100, Loss: 45.5940\n",
      "Epoch 60/100, Loss: 44.6816\n",
      "Epoch 61/100, Loss: 43.8021\n",
      "Epoch 62/100, Loss: 42.9550\n",
      "Epoch 63/100, Loss: 42.1393\n",
      "Epoch 64/100, Loss: 41.3534\n",
      "Epoch 65/100, Loss: 40.5968\n",
      "Epoch 66/100, Loss: 39.8685\n",
      "Epoch 67/100, Loss: 39.1674\n",
      "Epoch 68/100, Loss: 38.4930\n",
      "Epoch 69/100, Loss: 37.8439\n",
      "Epoch 70/100, Loss: 37.2196\n",
      "Epoch 71/100, Loss: 36.6190\n",
      "Epoch 72/100, Loss: 36.0413\n",
      "Epoch 73/100, Loss: 35.4858\n",
      "Epoch 74/100, Loss: 34.9517\n",
      "Epoch 75/100, Loss: 34.4380\n",
      "Epoch 76/100, Loss: 33.9440\n",
      "Epoch 77/100, Loss: 33.4691\n",
      "Epoch 78/100, Loss: 33.0124\n",
      "Epoch 79/100, Loss: 32.5732\n",
      "Epoch 80/100, Loss: 32.1508\n",
      "Epoch 81/100, Loss: 31.7446\n",
      "Epoch 82/100, Loss: 31.3538\n",
      "Epoch 83/100, Loss: 30.9779\n",
      "Epoch 84/100, Loss: 30.6163\n",
      "Epoch 85/100, Loss: 30.2682\n",
      "Epoch 86/100, Loss: 29.9332\n",
      "Epoch 87/100, Loss: 29.6106\n",
      "Epoch 88/100, Loss: 29.3000\n",
      "Epoch 89/100, Loss: 29.0008\n",
      "Epoch 90/100, Loss: 28.7125\n",
      "Epoch 91/100, Loss: 28.4346\n",
      "Epoch 92/100, Loss: 28.1668\n",
      "Epoch 93/100, Loss: 27.9085\n",
      "Epoch 94/100, Loss: 27.6593\n",
      "Epoch 95/100, Loss: 27.4189\n",
      "Epoch 96/100, Loss: 27.1868\n",
      "Epoch 97/100, Loss: 26.9626\n",
      "Epoch 98/100, Loss: 26.7462\n",
      "Epoch 99/100, Loss: 26.5370\n",
      "Epoch 100/100, Loss: 26.3348\n",
      "\n",
      "Final Loss after training: 26.139328002929688\n"
     ]
    }
   ],
   "source": [
    "final_loss = model.learn()\n",
    "print(\"\\nFinal Loss after training:\", final_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Drug Embeddings Shape: (Parameter containing:\n",
      "tensor([[-0.2483, -1.2082, -0.4777,  0.5201],\n",
      "        [ 1.6423, -0.1596, -0.4974,  0.4396],\n",
      "        [ 0.3189, -0.4245,  0.3057, -0.7746],\n",
      "        [ 0.0349,  0.3211,  1.5736, -0.8455],\n",
      "        [-1.2742,  2.1228, -1.2347, -0.4879],\n",
      "        [-1.4181,  0.8963,  0.0499,  2.2667]], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.4880,  1.1914, -0.8140, -0.7360],\n",
      "        [-0.8371, -0.9224,  1.8113,  0.1606],\n",
      "        [ 0.1971, -1.1441,  0.3383,  1.6992],\n",
      "        [ 0.0109, -0.3387, -1.3407, -0.5854],\n",
      "        [-0.5644,  1.0563, -1.4692,  1.4332],\n",
      "        [ 0.7440, -0.4816, -1.0495,  0.6039]], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.get_embeddings()\n",
    "print(\"\\nDrug Embeddings Shape:\", embeddings)  # Expect (n_drugs, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229, -0.1863],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "beta, w, v = model.get_params()\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([968, 3964])\n"
     ]
    }
   ],
   "source": [
    "def load_data(path_to_csv, device):\n",
    "    df = pd.read_csv(path_to_csv, index_col=0)\n",
    "    Aij = torch.tensor(df.values, dtype=torch.float32).to(device)\n",
    "    return Aij\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "csv_path = \"/Users/christine/Bachelor/src/data/adj_matrix.csv\" \n",
    "Aij = load_data(csv_path, device)\n",
    "print(Aij.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 181.0123\n",
      "Epoch 2/50, Loss: 176.5727\n",
      "Epoch 3/50, Loss: 172.0259\n",
      "Epoch 4/50, Loss: 167.6300\n",
      "Epoch 5/50, Loss: 161.9679\n",
      "Epoch 6/50, Loss: 157.7904\n",
      "Epoch 7/50, Loss: 151.7343\n",
      "Epoch 8/50, Loss: 147.1560\n",
      "Epoch 9/50, Loss: 142.7737\n",
      "Epoch 10/50, Loss: 137.9396\n",
      "Epoch 11/50, Loss: 132.0063\n",
      "Epoch 12/50, Loss: 127.6929\n",
      "Epoch 13/50, Loss: 122.8156\n",
      "Epoch 14/50, Loss: 118.3766\n",
      "Epoch 15/50, Loss: 113.9239\n",
      "Epoch 16/50, Loss: 109.6384\n",
      "Epoch 17/50, Loss: 105.4291\n",
      "Epoch 18/50, Loss: 101.4095\n",
      "Epoch 19/50, Loss: 97.4802\n",
      "Epoch 20/50, Loss: 93.7245\n",
      "Epoch 21/50, Loss: 90.1176\n",
      "Epoch 22/50, Loss: 86.6545\n",
      "Epoch 23/50, Loss: 83.3182\n",
      "Epoch 24/50, Loss: 80.1134\n",
      "Epoch 25/50, Loss: 77.0573\n",
      "Epoch 26/50, Loss: 74.1201\n",
      "Epoch 27/50, Loss: 71.3184\n",
      "Epoch 28/50, Loss: 68.6457\n",
      "Epoch 29/50, Loss: 66.0966\n",
      "Epoch 30/50, Loss: 63.6686\n",
      "Epoch 31/50, Loss: 61.3564\n",
      "Epoch 32/50, Loss: 59.1587\n",
      "Epoch 33/50, Loss: 57.0710\n",
      "Epoch 34/50, Loss: 55.0897\n",
      "Epoch 35/50, Loss: 53.2117\n",
      "Epoch 36/50, Loss: 51.4322\n",
      "Epoch 37/50, Loss: 49.7477\n",
      "Epoch 38/50, Loss: 48.1548\n",
      "Epoch 39/50, Loss: 46.6493\n",
      "Epoch 40/50, Loss: 45.2274\n",
      "Epoch 41/50, Loss: 43.8853\n",
      "Epoch 42/50, Loss: 42.6196\n",
      "Epoch 43/50, Loss: 41.4265\n",
      "Epoch 44/50, Loss: 40.3025\n",
      "Epoch 45/50, Loss: 39.2439\n",
      "Epoch 46/50, Loss: 38.2475\n",
      "Epoch 47/50, Loss: 37.3096\n",
      "Epoch 48/50, Loss: 36.4269\n",
      "Epoch 49/50, Loss: 35.5963\n",
      "Epoch 50/50, Loss: 34.8144\n"
     ]
    }
   ],
   "source": [
    "# Define thresholds for ordinal categories\n",
    "thresholds = torch.tensor([0, 1, 2, 3], dtype=torch.float).to(device)  # Adjust as needed\n",
    "\n",
    "# Define model hyperparameters\n",
    "embedding_dim = 5  # Number of dimensions in latent space\n",
    "n_epochs = 50\n",
    "lr = 0.01\n",
    "seed = 42\n",
    "\n",
    "# Initialize the model\n",
    "model = LDMprobit(Aij, thresholds, embedding_dim, device, n_epochs, lr, seed)\n",
    "\n",
    "# Train the model\n",
    "model.learn()\n",
    "\n",
    "# Get final learned embeddings\n",
    "drug_embeddings, side_effect_embeddings = model.get_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LDMprobit(torch.nn.Module):\n",
    "    def __init__(self, Aij, embedding_dim, device, n_epochs, lr, seed=None):\n",
    "        super(LDMprobit, self).__init__()\n",
    "        self.Aij = Aij.to(device)\n",
    "        self.device = device\n",
    "        self.n_drugs, self.n_effects = Aij.shape\n",
    "        self.n_ordinal_classes = Aij.max().int().item() +1\n",
    "\n",
    "        #set seed\n",
    "        self.seed = seed\n",
    "        self.__set_seed(seed)\n",
    "\n",
    "        #Variables for the learning process\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "\n",
    "        #parameters to be learned (latent representations)\n",
    "        self.beta = nn.Parameter(torch.randn(self.n_effects, device=device))\n",
    "        self.w = torch.nn.Parameter(torch.randn(self.n_drugs, embedding_dim))  # Latent embeddings for drugs\n",
    "        self.v = torch.nn.Parameter(torch.randn(self.n_effects, embedding_dim))  # Latent embeddings for side effects\n",
    "\n",
    "        # Parameters to be learned (thresholds)\n",
    "        self.beta_thilde = nn.Parameter(torch.randn(self.n_ordinal_classes, device=device))\n",
    "        self.a = nn.Parameter(torch.rand(1, device=device))\n",
    "        self.b = nn.Parameter(torch.rand(1, device=device))\n",
    "    \n",
    "    def __set_seed(self, seed):\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        return self.w, self.v\n",
    "    \n",
    "    # def get_thresholds(self):\n",
    "    #     softmax_values = torch.softmax(self.beta_thilde, dim=0)  # Ensure positive values\n",
    "    #     scaled_thresholds = torch.cumsum(softmax_values, dim=0) * torch.abs(self.a)  # Ensure increasing values\n",
    "    #     positive_thresholds = scaled_thresholds + torch.abs(self.b)  # Shift to ensure positivity\n",
    "    #     return torch.cat([torch.tensor([-float(\"inf\")], device=self.device), positive_thresholds, torch.tensor([float(\"inf\")], device=self.device)])\n",
    "    def get_thresholds(self):\n",
    "        # Ensure thresholds remain positive and increasing\n",
    "        deltas = torch.softmax(self.beta_thilde, dim = 0)  # Ensure positive increments\n",
    "        thresholds = torch.cumsum(deltas, dim=0)* self.a - self.b\n",
    "        return torch.cat([torch.tensor([-float(\"inf\")], device=self.device), thresholds, torch.tensor([float(\"inf\")], device=self.device)])\n",
    "    \n",
    "    def probit(self):\n",
    "        # #n_ordinal_classes = len(self.thresholds) - 1\n",
    "        # #n_drugs, n_effects = self.Aij.shape\n",
    "        normal_dist = Normal(0, 1) # Noise contaminated by normal distribution\n",
    "        probit_matrix = torch.zeros((self.n_ordinal_classes, self.n_drugs, self.n_effects), device=self.device)\n",
    "        thresholds = self.get_thresholds()\n",
    "    \n",
    "        #Linear term (\\beta^T x_{i,j})\n",
    "        linear_term = torch.matmul(self.Aij, self.beta.unsqueeze(1))\n",
    "\n",
    "        # Distance term -|w_i - v_j|\n",
    "        dist = -torch.norm(self.w.unsqueeze(1) - self.v.unsqueeze(0), dim=2)\n",
    "\n",
    "        # Latent variable \\beta^T x_{i,j} + \\alpha(u_i - u_j)\n",
    "        latent_var = linear_term + dist\n",
    "        \n",
    "        for y in range(self.n_ordinal_classes):\n",
    "            z1 = latent_var - thresholds[y]\n",
    "            z2 = latent_var - thresholds[y+1]\n",
    "            probit_matrix[y, :, :] = normal_dist.cdf(z1) - normal_dist.cdf(z2)\n",
    "        return probit_matrix\n",
    "\n",
    "    \n",
    "    def predict_categories(self):\n",
    "        probit_matrix = self.probit()  # Call probit to get probabilities\n",
    "        return torch.argmax(probit_matrix, dim=0), probit_matrix\n",
    "    \n",
    "    def ordinal_cross_entropy_loss(self):\n",
    "    # Compute the predicted probabilities using the probit function\n",
    "        probit_matrix = self.probit() \n",
    "\n",
    "        # Initialize loss variable\n",
    "        loss = 0.0\n",
    "\n",
    "        # Iterate over all drugs and side effects\n",
    "        for i in range(self.n_drugs):  # For each drug\n",
    "            for j in range(self.n_effects):  # For each side effect\n",
    "                if True:#self.Aij[i, j] != 0:  # Only compute loss for nonzero entries\n",
    "                    target_class = int(self.Aij[i, j])  # Convert severity to class (0-based)\n",
    "                    \n",
    "                    #One-hot encode target \n",
    "                    one_hot_target = torch.zeros(self.n_ordinal_classes, device=self.device)\n",
    "                    one_hot_target[target_class] = 1  # Set the correct class to 1\n",
    "\n",
    "                    # Compute the log-likelihood \n",
    "                    prob = probit_matrix[:,i,j]\n",
    "                    loss -= torch.log(torch.sum(prob * one_hot_target) + 1e-8)  # Negative log-likelihood, addition of small number to avoid log(0)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        final_loss = None  # Store the last loss\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            loss = self.ordinal_cross_entropy_loss()  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update parameters\n",
    "            \n",
    "            final_loss = loss.item()  # Store latest loss value\n",
    "            \n",
    "            if epoch % 10 == 0:  # Print every 10 epochs\n",
    "                print(f\"Epoch {epoch}/{self.n_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        return final_loss\n",
    "    def plot_network(self):\n",
    "        G = nx.Graph()\n",
    "\n",
    "        # Get embeddings (drug and effect embeddings)\n",
    "        drug_embeddings = self.w.detach().cpu().numpy()  # (n_drugs, embedding_dim)\n",
    "        effect_embeddings = self.v.detach().cpu().numpy()  # (n_effects, embedding_dim)\n",
    "\n",
    "        # Add nodes for drugs and side effects\n",
    "        for i in range(self.n_drugs):\n",
    "            G.add_node(f\"Drug_{i}\", bipartite=0)\n",
    "        for j in range(self.n_effects):\n",
    "            G.add_node(f\"Effect_{j}\", bipartite=1)\n",
    "\n",
    "        # Calculate the probability matrix using the probit function\n",
    "        probit_matrix = self.probit()\n",
    "\n",
    "        # Add edges based on the probit matrix (non-zero probability indicates a link)\n",
    "        for i in range(self.n_drugs):\n",
    "            for j in range(self.n_effects):\n",
    "                prob = probit_matrix[i, j].max().item()  # Use the max probability for this drug-effect pair\n",
    "                if prob > 0.01:  # Threshold for displaying an edge\n",
    "                    G.add_edge(f\"Drug_{i}\", f\"Effect_{j}\", weight=prob)\n",
    "\n",
    "        # Create a layout based on the embeddings\n",
    "        pos = {}\n",
    "        \n",
    "        # Position drugs based on their embeddings\n",
    "        for i in range(self.n_drugs):\n",
    "            pos[f\"Drug_{i}\"] = (drug_embeddings[i, 0], drug_embeddings[i, 1])  # 2D position based on first two embedding dims\n",
    "        \n",
    "        # Position effects based on their embeddings\n",
    "        for j in range(self.n_effects):\n",
    "            pos[f\"Effect_{j}\"] = (effect_embeddings[j, 0], effect_embeddings[j, 1])\n",
    "\n",
    "        # Draw the graph\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        nx.draw(G, pos, with_labels=True, node_size=500, node_color=[\"blue\" if \"Drug\" in node else \"red\" for node in G.nodes], font_size=10, font_weight='bold', edge_color='gray')\n",
    "\n",
    "        # Display edge weights (probabilities) as labels\n",
    "        # labels = nx.get_edge_attributes(G, 'weight')\n",
    "        # nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "\n",
    "        plt.title('Drug-Side Effect Network based on Embeddings and Probit Output')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_links(self):\n",
    "        G = nx.Graph()\n",
    "\n",
    "        # Add nodes for drugs and side effects\n",
    "        for i in range(self.n_drugs):\n",
    "            G.add_node(f\"Drug_{i}\", bipartite=0)\n",
    "        for j in range(self.n_effects):\n",
    "            G.add_node(f\"Effect_{j}\", bipartite=1)\n",
    "\n",
    "        # Calculate the probability matrix using the probit function\n",
    "        probit_matrix = self.probit()\n",
    "\n",
    "        # Add edges based on the probit matrix (non-zero probability indicates a link)\n",
    "        for i in range(self.n_drugs):\n",
    "            for j in range(self.n_effects):\n",
    "                prob = probit_matrix[i, j].max().item()  # Use the max probability for this drug-effect pair\n",
    "                if prob > 0.35:  # Threshold for displaying an edge\n",
    "                    G.add_edge(f\"Drug_{i}\", f\"Effect_{j}\", weight=prob)\n",
    "\n",
    "        pos = {}\n",
    "        pos.update((node, (1, index)) for index, node in enumerate(f\"Drug_{i}\" for i in range(self.n_drugs)))  # Position for drugs\n",
    "        pos.update((node, (2, index)) for index, node in enumerate(f\"Effect_{j}\" for j in range(self.n_effects)))  # Position for effects\n",
    "\n",
    "        # Draw the graph\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        nx.draw(G, pos, with_labels=True, node_size=500, node_color='skyblue', font_size=10, font_weight='bold', edge_color='gray')\n",
    "\n",
    "        # Display edge weights (optional)\n",
    "        labels = nx.get_edge_attributes(G, 'weight')\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "\n",
    "        plt.title('Drug-Side Effect Network')\n",
    "        plt.show()\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.beta, self.w.detach().cpu().numpy(), self.v.detach().cpu().numpy(), self.beta_thilde.detach().cpu().numpy()\n",
    "        \n",
    "    def save_embeddings():\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500, Loss: 207.0220\n",
      "Epoch 10/500, Loss: 193.6110\n",
      "Epoch 20/500, Loss: 171.1986\n",
      "Epoch 30/500, Loss: 133.7646\n",
      "Epoch 40/500, Loss: 93.9879\n",
      "Epoch 50/500, Loss: 63.5178\n",
      "Epoch 60/500, Loss: 48.1958\n",
      "Epoch 70/500, Loss: 41.1482\n",
      "Epoch 80/500, Loss: 36.7619\n",
      "Epoch 90/500, Loss: 33.6167\n",
      "Epoch 100/500, Loss: 31.3641\n",
      "Epoch 110/500, Loss: 29.6154\n",
      "Epoch 120/500, Loss: 28.1393\n",
      "Epoch 130/500, Loss: 26.8318\n",
      "Epoch 140/500, Loss: 25.6392\n",
      "Epoch 150/500, Loss: 24.5358\n",
      "Epoch 160/500, Loss: 23.5083\n",
      "Epoch 170/500, Loss: 22.5495\n",
      "Epoch 180/500, Loss: 21.6552\n",
      "Epoch 190/500, Loss: 20.8219\n",
      "Epoch 200/500, Loss: 20.0465\n",
      "Epoch 210/500, Loss: 19.3259\n",
      "Epoch 220/500, Loss: 18.6565\n",
      "Epoch 230/500, Loss: 18.0351\n",
      "Epoch 240/500, Loss: 17.4580\n",
      "Epoch 250/500, Loss: 16.9216\n",
      "Epoch 260/500, Loss: 16.4225\n",
      "Epoch 270/500, Loss: 15.9572\n",
      "Epoch 280/500, Loss: 15.5225\n",
      "Epoch 290/500, Loss: 15.1156\n",
      "Epoch 300/500, Loss: 14.7336\n",
      "Epoch 310/500, Loss: 14.3743\n",
      "Epoch 320/500, Loss: 14.0353\n",
      "Epoch 330/500, Loss: 13.7148\n",
      "Epoch 340/500, Loss: 13.4110\n",
      "Epoch 350/500, Loss: 13.1225\n",
      "Epoch 360/500, Loss: 12.8479\n",
      "Epoch 370/500, Loss: 12.5860\n",
      "Epoch 380/500, Loss: 12.3359\n",
      "Epoch 390/500, Loss: 12.0964\n",
      "Epoch 400/500, Loss: 11.8669\n",
      "Epoch 410/500, Loss: 11.6466\n",
      "Epoch 420/500, Loss: 11.4348\n",
      "Epoch 430/500, Loss: 11.2309\n",
      "Epoch 440/500, Loss: 11.0345\n",
      "Epoch 450/500, Loss: 10.8449\n",
      "Epoch 460/500, Loss: 10.6617\n",
      "Epoch 470/500, Loss: 10.4846\n",
      "Epoch 480/500, Loss: 10.3132\n",
      "Epoch 490/500, Loss: 10.1471\n",
      "10.001927375793457\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_dim = 5\n",
    "Aij = torch.tensor([[0, 2, 0, 3, 1, 2], \n",
    "                    [0, 0, 2, 0, 1, 0],\n",
    "                    [3, 3, 0, 0, 0, 1],\n",
    "                    [3, 3, 0, 0, 0, 1],\n",
    "                    [0, 0, 2, 0, 0, 0]],dtype=torch.float32) #column: side effect, row:drug, value: frequency (ordinal)\n",
    "n_epochs = 500\n",
    "lr = 0.01\n",
    "seed = 20\n",
    "model = LDMprobit(Aij, embedding_dim, device, n_epochs, lr, seed)\n",
    "probit_output = model.probit()  # Compute the probit probability matrix\n",
    "loss_out = model.train()  # Compute the ordinal cross-entropy loss\n",
    "print(loss_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   -inf, -1.1659, -0.0777,  1.1145,  2.9408,     inf],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_thresholds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal matrice\n",
      "tensor([[0., 2., 0., 3., 1., 2.],\n",
      "        [0., 0., 2., 0., 1., 0.],\n",
      "        [3., 3., 0., 0., 0., 1.],\n",
      "        [3., 3., 0., 0., 0., 1.],\n",
      "        [0., 0., 2., 0., 0., 0.]])\n",
      "Obtained through loss\n",
      "(tensor([[0, 2, 0, 3, 1, 2],\n",
      "        [0, 0, 2, 0, 1, 0],\n",
      "        [3, 3, 0, 0, 0, 1],\n",
      "        [3, 3, 0, 0, 0, 1],\n",
      "        [0, 0, 2, 0, 0, 0]]), tensor([[[7.7926e-01, 1.1680e-01, 9.4371e-01, 2.3836e-03, 2.6842e-01,\n",
      "          4.6688e-02],\n",
      "         [9.9986e-01, 9.9079e-01, 5.7178e-02, 9.7995e-01, 3.4095e-01,\n",
      "          9.6395e-01],\n",
      "         [1.3719e-03, 2.3763e-03, 9.9507e-01, 9.0207e-01, 9.7478e-01,\n",
      "          2.7960e-01],\n",
      "         [1.4328e-03, 2.1979e-03, 9.9363e-01, 8.9940e-01, 9.7071e-01,\n",
      "          2.7990e-01],\n",
      "         [9.9994e-01, 9.9559e-01, 5.7039e-02, 9.9951e-01, 9.5587e-01,\n",
      "          9.9863e-01]],\n",
      "\n",
      "        [[1.8915e-01, 3.4223e-01, 5.2550e-02, 3.9065e-02, 4.1263e-01,\n",
      "          2.3104e-01],\n",
      "         [1.4058e-04, 8.9287e-03, 2.5465e-01, 1.9209e-02, 4.1028e-01,\n",
      "          3.4108e-02],\n",
      "         [2.6899e-02, 3.8985e-02, 4.8116e-03, 8.9311e-02, 2.4053e-02,\n",
      "          4.1334e-01],\n",
      "         [2.7708e-02, 3.7003e-02, 6.2016e-03, 9.1622e-02, 2.7847e-02,\n",
      "          4.1336e-01],\n",
      "         [6.4403e-05, 4.3004e-03, 2.5436e-01, 4.8769e-04, 4.1515e-02,\n",
      "          1.3470e-03]],\n",
      "\n",
      "        [[3.0446e-02, 4.0295e-01, 3.6817e-03, 2.5247e-01, 2.7077e-01,\n",
      "          4.4886e-01],\n",
      "         [1.1921e-06, 2.8366e-04, 4.4666e-01, 8.3476e-04, 2.1806e-01,\n",
      "          1.9241e-03],\n",
      "         [2.0912e-01, 2.5221e-01, 1.2133e-04, 8.4406e-03, 1.1542e-03,\n",
      "          2.6215e-01],\n",
      "         [2.1237e-01, 2.4585e-01, 1.7160e-04, 8.7953e-03, 1.4282e-03,\n",
      "          2.6192e-01],\n",
      "         [4.4703e-07, 1.0419e-04, 4.4671e-01, 5.8711e-06, 2.5780e-03,\n",
      "          2.2084e-05]],\n",
      "\n",
      "        [[1.1435e-03, 1.3624e-01, 5.5075e-05, 6.0658e-01, 4.7935e-02,\n",
      "          2.6584e-01],\n",
      "         [0.0000e+00, 1.7583e-06, 2.3577e-01, 7.3612e-06, 3.0597e-02,\n",
      "          2.2650e-05],\n",
      "         [6.2946e-01, 6.0674e-01, 5.9605e-07, 1.7589e-04, 1.1355e-05,\n",
      "          4.4697e-02],\n",
      "         [6.2817e-01, 6.1083e-01, 9.2387e-07, 1.8650e-04, 1.5140e-05,\n",
      "          4.4613e-02],\n",
      "         [0.0000e+00, 4.7684e-07, 2.3613e-01, 0.0000e+00, 3.3736e-05,\n",
      "          5.9605e-08]]], grad_fn=<CopySlices>))\n",
      "Embeddings: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[-1.8760, -1.1874,  0.8197,  0.1494, -1.7796],\n",
       "         [-0.2435,  3.0271, -0.9507, -1.3117, -1.1426],\n",
       "         [-0.4712, -2.4295, -0.7742,  0.5608,  1.4998],\n",
       "         [-0.5940, -2.3083, -0.8633,  0.4821,  1.5446],\n",
       "         [ 2.0860,  2.7021, -2.3055,  0.5721, -1.8318]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.3805, -2.4470, -0.4952,  0.5661,  2.5484],\n",
       "         [-0.8714, -2.0413, -0.7425,  0.4219,  0.3779],\n",
       "         [ 0.4108,  3.3098, -1.9245, -0.2011, -1.4660],\n",
       "         [-2.3958, -0.5789,  1.6886,  0.0518, -2.4329],\n",
       "         [-1.1747,  2.0305,  1.4991, -0.9615, -1.4354],\n",
       "         [-0.7426, -1.0268,  1.7253, -0.7753,  0.0243]], requires_grad=True))"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Goal matrice\")\n",
    "print(Aij)\n",
    "print(\"Obtained through loss\")\n",
    "print(model.predict_categories())\n",
    "print(\"Embeddings: \")\n",
    "model.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for dimension 1 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[212], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[207], line 163\u001b[0m, in \u001b[0;36mLDMprobit.plot_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_drugs):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_effects):\n\u001b[0;32m--> 163\u001b[0m         prob \u001b[38;5;241m=\u001b[39m \u001b[43mprobit_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Use the max probability for this drug-effect pair\u001b[39;00m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.01\u001b[39m:  \u001b[38;5;66;03m# Threshold for displaying an edge\u001b[39;00m\n\u001b[1;32m    165\u001b[0m             G\u001b[38;5;241m.\u001b[39madd_edge(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDrug_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEffect_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, weight\u001b[38;5;241m=\u001b[39mprob)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for dimension 1 with size 5"
     ]
    }
   ],
   "source": [
    "model.plot_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "\n",
    "    \n",
    "    def plot_network(self):\n",
    "        G = nx.Graph()\n",
    "\n",
    "        # Get embeddings (drug and effect embeddings)\n",
    "        drug_embeddings = self.w.detach().cpu().numpy()  # (n_drugs, embedding_dim)\n",
    "        effect_embeddings = self.v.detach().cpu().numpy()  # (n_effects, embedding_dim)\n",
    "\n",
    "        # Add nodes for drugs and side effects\n",
    "        for i in range(self.n_drugs):\n",
    "            G.add_node(f\"Drug_{i}\", bipartite=0)\n",
    "        for j in range(self.n_effects):\n",
    "            G.add_node(f\"Effect_{j}\", bipartite=1)\n",
    "\n",
    "        # Calculate the probability matrix using the probit function\n",
    "        probit_matrix = self.probit()\n",
    "\n",
    "        # Add edges based on the probit matrix (non-zero probability indicates a link)\n",
    "        for i in range(self.n_drugs):\n",
    "            for j in range(self.n_effects):\n",
    "                prob = probit_matrix[i, j].max().item()  # Use the max probability for this drug-effect pair\n",
    "                if prob > 0.01:  # Threshold for displaying an edge\n",
    "                    G.add_edge(f\"Drug_{i}\", f\"Effect_{j}\", weight=prob)\n",
    "\n",
    "        # Create a layout based on the embeddings\n",
    "        pos = {}\n",
    "        \n",
    "        # Position drugs based on their embeddings\n",
    "        for i in range(self.n_drugs):\n",
    "            pos[f\"Drug_{i}\"] = (drug_embeddings[i, 0], drug_embeddings[i, 1])  # 2D position based on first two embedding dims\n",
    "        \n",
    "        # Position effects based on their embeddings\n",
    "        for j in range(self.n_effects):\n",
    "            pos[f\"Effect_{j}\"] = (effect_embeddings[j, 0], effect_embeddings[j, 1])\n",
    "\n",
    "        # Draw the graph\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        nx.draw(G, pos, with_labels=True, node_size=500, node_color=[\"blue\" if \"Drug\" in node else \"red\" for node in G.nodes], font_size=10, font_weight='bold', edge_color='gray')\n",
    "\n",
    "        # Display edge weights (probabilities) as labels\n",
    "        # labels = nx.get_edge_attributes(G, 'weight')\n",
    "        # nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "\n",
    "        plt.title('Drug-Side Effect Network based on Embeddings and Probit Output')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_links(self):\n",
    "        G = nx.Graph()\n",
    "\n",
    "        # Add nodes for drugs and side effects\n",
    "        for i in range(self.n_drugs):\n",
    "            G.add_node(f\"Drug_{i}\", bipartite=0)\n",
    "        for j in range(self.n_effects):\n",
    "            G.add_node(f\"Effect_{j}\", bipartite=1)\n",
    "\n",
    "        # Calculate the probability matrix using the probit function\n",
    "        probit_matrix = self.probit()\n",
    "\n",
    "        # Add edges based on the probit matrix (non-zero probability indicates a link)\n",
    "        for i in range(self.n_drugs):\n",
    "            for j in range(self.n_effects):\n",
    "                prob = probit_matrix[i, j].max().item()  # Use the max probability for this drug-effect pair\n",
    "                if prob > 0.7:  # Threshold for displaying an edge\n",
    "                    G.add_edge(f\"Drug_{i}\", f\"Effect_{j}\", weight=prob)\n",
    "\n",
    "        pos = {}\n",
    "        pos.update((node, (1, index)) for index, node in enumerate(f\"Drug_{i}\" for i in range(self.n_drugs)))  # Position for drugs\n",
    "        pos.update((node, (2, index)) for index, node in enumerate(f\"Effect_{j}\" for j in range(self.n_effects)))  # Position for effects\n",
    "\n",
    "        # Draw the graph\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        nx.draw(G, pos, with_labels=True, node_size=500, node_color='skyblue', font_size=10, font_weight='bold', edge_color='gray')\n",
    "\n",
    "        # Display edge weights (optional)\n",
    "        labels = nx.get_edge_attributes(G, 'weight')\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "\n",
    "        plt.title('Drug-Side Effect Network')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
